{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6011fe0b",
   "metadata": {},
   "source": [
    "# **Demostración de Técnicas de Preprocesamiento de Texto en PNL**\n",
    "\n",
    "## **Objetivo del Script:**\n",
    "Este script muestra diversas técnicas comunes de preprocesamiento de texto utilizadas en el Procesamiento del Lenguaje Natural (PNL). Cada técnica se ilustra con texto de ejemplo, aplicando la transformación y mostrando los resultados. Este script fue adaptado de  \"Complete Guide to Text Preprocessing in NLP\" (https://medium.com/@devangchavan0204/complete-guide-to-text-preprocessing-in-nlp-b4092c104d3e).\n",
    "\n",
    "## **Estructura del Script:**\n",
    "1.  Carga de las librerías necesarias y descarga de recursos NLTK (si es necesario).\n",
    "2.  Definición de texto de ejemplo.\n",
    "3.  Aplicación y visualización de técnicas de preprocesamiento:\n",
    "    * Minúsculas (Lowercasing)\n",
    "    * Eliminar Etiquetas HTML (Remove HTML Tags)\n",
    "    * Eliminar URLs (Remove URLs)\n",
    "    * Eliminar Puntuación (Remove Punctuations)\n",
    "    * Tratamiento de Palabras de Chat (Chat Word Treatment)\n",
    "    * Corrección Ortográfica (Spelling Correction)\n",
    "    * Eliminación de Palabras Vacías (Removing Stop Words)\n",
    "    * Manejo de Emojis (Handling Emojis)\n",
    "    * Tokenización (Tokenization)\n",
    "    * Derivación (Stemming)\n",
    "    * Lematización (Lemmatization)\n",
    "\n",
    "## **Nota sobre NLTK:**\n",
    "Algunas funciones de NLTK requieren la descarga de recursos específicos (corpus, modelos). Si es la primera vez que usas NLTK para estas tareas, es posible que necesites ejecutar comandos de descarga. Se incluye una celda para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23ee3b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías cargadas.\n",
      "Recursos NLTK necesarios parecen estar disponibles.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Carga de Librerías y Descarga de Recursos NLTK\n",
    "# ------------------------------------------------------------------------------\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd # Opcional, para mostrar cómo se aplicaría a un DataFrame\n",
    "\n",
    "# Descomenta y ejecuta las siguientes líneas si no tienes los recursos de NLTK\n",
    "# Necesitarás hacerlo una vez por instalación de Python/entorno.\n",
    "nltk.download('punkt') # Para tokenización\n",
    "nltk.download('stopwords') # Para palabras vacías\n",
    "nltk.download('wordnet') # Para lematización\n",
    "nltk.download('omw-1.4') # Necesario para wordnet en algunos casos\n",
    "nltk.download('punkt_tab') # Para tokenización de tablas\n",
    "\n",
    "print(\"Librerías cargadas.\")\n",
    "try:\n",
    "    # Intenta cargar recursos para verificar si están disponibles\n",
    "    stopwords.words('english')\n",
    "    word_tokenize(\"test\")\n",
    "    PorterStemmer().stem(\"testing\")\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "    print(\"Recursos NLTK necesarios parecen estar disponibles.\")\n",
    "except LookupError as e:\n",
    "    print(f\"Error de NLTK: {e}. Por favor, descomenta y ejecuta las líneas nltk.download(...) en el script.\")\n",
    "except Exception as e:\n",
    "    print(f\"Otro error relacionado con NLTK: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c74abd",
   "metadata": {},
   "source": [
    "## **2. Texto de Ejemplo**\n",
    "\n",
    "Definiremos algunas cadenas de texto para aplicar las técnicas de preprocesamiento.\n",
    "Estas muestras incluyen características que cada paso de preprocesamiento abordará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8f9a98",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de ejemplo original:\n",
      "1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "4: Thiss is a sentance with some missspellings and errrors. :D 😂\n",
      "5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "6: I love programming, programmers, and programs. Walking, walked, walks.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\",\n",
    "    \"Check out my new blog post at https://example.com/my-post or www.another-example.com.\",\n",
    "    \"OMG! This is GR8! BTW, what's up? CUL8R.\",\n",
    "    \"Thiss is a sentance with some missspellings and errrors. :D 😂\",\n",
    "    \"The quick brown fox jumps over the lazy dog. He was running and eating.\",\n",
    "    \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "]\n",
    "\n",
    "# También podemos crear un DataFrame de Pandas para simular la aplicación a un conjunto de datos,\n",
    "# como se menciona en el artículo de referencia con el dataset IMDB.\n",
    "df_sample = pd.DataFrame(sample_texts, columns=['text'])\n",
    "\n",
    "print(\"Texto de ejemplo original:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88749b",
   "metadata": {},
   "source": [
    "## **3. Técnicas de Preprocesamiento de Texto**\n",
    "\n",
    "A continuación, aplicaremos cada técnica de preprocesamiento.\n",
    "No siempre es necesario aplicar todos los pasos; la elección depende de los requisitos del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c38e8",
   "metadata": {},
   "source": [
    "### **3.1 Minúsculas (Lowercasing)**\n",
    "Convierte todo el texto a minúsculas para asegurar uniformidad.\n",
    "Por ejemplo, \"Word\" y \"word\" se tratan como la misma entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cf32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1 Minúsculas\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: hello world! this is an <b>example</b> text for nlp preprocessing.\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: omg! this is gr8! btw, what's up? cul8r.\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D 😂\n",
      "Procesado 4: thiss is a sentance with some missspellings and errrors. :d 😂\n",
      "\n",
      "Original 5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "Procesado 5: the quick brown fox jumps over the lazy dog. he was running and eating.\n",
      "\n",
      "Original 6: I love programming, programmers, and programs. Walking, walked, walks.\n",
      "Procesado 6: i love programming, programmers, and programs. walking, walked, walks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower() # String = representacion del texto en el lenguaje de programación\n",
    "\n",
    "print(\"3.1 Minúsculas\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = lowercase_text(text)\n",
    "    if text != processed_text: # Mostrar solo si hay cambios\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")\n",
    "\n",
    "# Aplicado al DataFrame de ejemplo:\n",
    "# df_sample['text_lowercased'] = df_sample['text'].apply(lowercase_text)\n",
    "# print(df_sample[['text', 'text_lowercased']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e65f66",
   "metadata": {},
   "source": [
    "### **3.2 Eliminar Etiquetas HTML (Remove HTML Tags)**\n",
    "Las etiquetas HTML, a menudo presentes en datos extraídos de la web (scraped data), son irrelevantes para los modelos de ML.\n",
    "Eliminarlas asegura que el modelo se enfoque en el contenido textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01cd569",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 Eliminar Etiquetas HTML\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: Hello WORLD! This is an example text for NLP preprocessing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>') # Expresión regular para encontrar etiquetas HTML\n",
    "    return pattern.sub(r'', text) # Reemplaza las etiquetas con una cadena vacía\n",
    "\n",
    "print(\"3.2 Eliminar Etiquetas HTML\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_html_tags(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08df9",
   "metadata": {},
   "source": [
    "### **3.3 Eliminar URLs (Remove URLs)**\n",
    "Las URLs pueden no aportar información valiosa y pueden confundir al modelo.\n",
    "Eliminarlas simplifica los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba33627",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 Eliminar URLs\n",
      "Original URL: For notebook click https://www.kaggle.com/campusx/notebook8223fclabb or visit www.mysite.com\n",
      "Procesado URL: For notebook click [URL] or visit [URL]\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: Check out my new blog post at [URL] or [URL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    # Patrón mejorado para cubrir http, https y www\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'[URL]', text)\n",
    "\n",
    "print(\"3.3 Eliminar URLs\")\n",
    "text_with_url = 'For notebook click https://www.kaggle.com/campusx/notebook8223fclabb or visit www.mysite.com' \n",
    "print(f\"Original URL: {text_with_url}\")\n",
    "print(f\"Procesado URL: {remove_urls(text_with_url)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_urls(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa2f2b",
   "metadata": {},
   "source": [
    "### **3.4 Eliminar Puntuación (Remove Punctuations)**\n",
    "Elimina caracteres como comas y puntos, enfocándose en las palabras.\n",
    "Puede ser beneficioso para tareas como el análisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 Eliminar Puntuación\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: Hello WORLD This is an BexampleB text for NLP preprocessing\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: Check out my new blog post at httpsexamplecommypost or wwwanotherexamplecom\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: OMG This is GR8 BTW whats up CUL8R\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D 😂\n",
      "Procesado 4: Thiss is a sentance with some missspellings and errrors D 😂\n",
      "\n",
      "Original 5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "Procesado 5: The quick brown fox jumps over the lazy dog He was running and eating\n",
      "\n",
      "Original 6: I love programming, programmers, and programs. Walking, walked, walks.\n",
      "Procesado 6: I love programming programmers and programs Walking walked walks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    # string.punctuation contiene !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ (Bag of word, tf-idf, word2vec)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "print(\"3.4 Eliminar Puntuación\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_punctuation(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830630f",
   "metadata": {},
   "source": [
    "### **3.5 Tratamiento de Palabras de Chat (Chat Word Treatment)**\n",
    "Convierte palabras de chat o jerga (ej. \"FYI\", \"LOL\") a sus formas completas para mejorar la comprensión del modelo.\n",
    "Se utiliza un diccionario para las conversiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d0e37b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 Tratamiento de Palabras de Chat\n",
      "Original Slang: FYI U R GR8! CUL8R.\n",
      "Procesado Slang: FOR YOUR INFORMATION YOU ARE GREAT! SEE YOU LATER\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: OH MY GOD This is GREAT! BY THE WAY what's up? SEE YOU LATER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de ejemplo\n",
    "chat_word_dict = {\n",
    "    'AFAIK': 'As Far As I Know', 'ASAP': 'As Soon As Possible',\n",
    "    'BTW': 'By The Way', 'B4': 'Before', 'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FYI': 'For Your Information',\n",
    "    'GG': 'Good Game', 'GR8': 'Great!', 'IMHO': 'In My Honest Opinion',\n",
    "    'LOL': 'Laughing Out Loud', 'OMG': 'Oh My God', 'TTYL': 'Talk To You Later',\n",
    "    'U': 'You', 'R': 'Are'\n",
    "}\n",
    "\n",
    "def chat_conversion(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Considerar la puntuación pegada a la palabra\n",
    "        word_upper = word.upper()\n",
    "        # Eliminar puntuación común al final para buscar en el dict\n",
    "        cleaned_word_upper = re.sub(r'[^\\w\\s]$', '', word_upper)\n",
    "\n",
    "        if cleaned_word_upper in chat_word_dict:\n",
    "            # Conservar la capitalización original (aproximado)\n",
    "            replacement = chat_word_dict[cleaned_word_upper]\n",
    "            if word.islower():\n",
    "                new_words.append(replacement.lower())\n",
    "            elif word.isupper():\n",
    "                new_words.append(replacement.upper())\n",
    "            elif word[0].isupper():\n",
    "                new_words.append(replacement.capitalize())\n",
    "            else:\n",
    "                new_words.append(replacement)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "print(\"3.5 Tratamiento de Palabras de Chat\")\n",
    "text_slang_example = \"FYI U R GR8! CUL8R.\"\n",
    "print(f\"Original Slang: {text_slang_example}\")\n",
    "print(f\"Procesado Slang: {chat_conversion(text_slang_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = chat_conversion(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99477",
   "metadata": {},
   "source": [
    "### **3.6 Corrección Ortográfica (Spelling Correction)**\n",
    "Corrige errores tipográficos e inexactitudes, mejorando la calidad de los datos.\n",
    "Se puede usar la librería `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602a0b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6 Corrección Ortográfica\n",
      "Original Incorrecto: Thiss is a sentance with some missspellings and errrors.\n",
      "Corregido (TextBlob): Hiss is a sentence with some missspellings and errors.\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D 😂\n",
      "Corregido 4: Hiss is a sentence with some missspellings and errors. :D 😂\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def correct_spelling(text):\n",
    "    # TextBlob es útil pero puede ser lento en grandes datasets\n",
    "    # y no siempre es perfecto.\n",
    "    text_blob = TextBlob(text)\n",
    "    return str(text_blob.correct())\n",
    "\n",
    "print(\"3.6 Corrección Ortográfica\")\n",
    "incorrect_text_example = 'Thiss is a sentance with some missspellings and errrors.'\n",
    "# Nota: La corrección de TextBlob puede no ser perfecta o puede cambiar palabras correctamente escritas.\n",
    "# Para este ejemplo, mostramos su funcionamiento básico.\n",
    "# En la práctica, se debe usar con precaución y evaluar su impacto.\n",
    "print(f\"Original Incorrecto: {incorrect_text_example}\")\n",
    "print(f\"Corregido (TextBlob): {correct_spelling(incorrect_text_example)}\\n\")\n",
    "\n",
    "# Aplicar a uno de los textos de muestra que tiene errores\n",
    "for i, text in enumerate(sample_texts):\n",
    "    if \"missspellings\" in text or \"Thiss\" in text: # Aplicar solo a textos con errores conocidos\n",
    "        processed_text = correct_spelling(text)\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Corregido {i+1}: {processed_text}\\n\")\n",
    "        break # Solo un ejemplo para no tardar mucho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8eadd",
   "metadata": {},
   "source": [
    "### **3.7 Eliminación de Palabras Vacías (Removing Stop Words)**\n",
    "Las palabras vacías (stop words) son palabras comunes que ayudan a formar oraciones pero aportan poco significado (ej. \"the\", \"is\", \"and\").\n",
    "NLTK proporciona una lista de estas palabras.\n",
    "Este paso no siempre es deseable, por ejemplo, en el etiquetado de Partes de la Oración (POS tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9ca79e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 Eliminación de Palabras Vacías\n",
      "Original con Stop Words: This is a really great time for the field of AI. It is advancing exponentially.\n",
      "Sin Stop Words: really great time field AI . advancing exponentially .\n",
      "\n",
      "Original 1 (limpio, tokenizado): hello world this is an bexampleb text for nlp preprocessing\n",
      "Sin Stop Words 1: hello world bexampleb text nlp preprocessing\n",
      "\n",
      "Original 2 (limpio, tokenizado): check out my new blog post at httpsexamplecommypost or wwwanotherexamplecom\n",
      "Sin Stop Words 2: check new blog post httpsexamplecommypost wwwanotherexamplecom\n",
      "\n",
      "Original 3 (limpio, tokenizado): omg this is gr8 btw whats up cul8r\n",
      "Sin Stop Words 3: omg gr8 btw whats cul8r\n",
      "\n",
      "Original 4 (limpio, tokenizado): thiss is a sentance with some missspellings and errrors d 😂\n",
      "Sin Stop Words 4: thiss sentance missspellings errrors 😂\n",
      "\n",
      "Original 5 (limpio, tokenizado): the quick brown fox jumps over the lazy dog he was running and eating\n",
      "Sin Stop Words 5: quick brown fox jumps lazy dog running eating\n",
      "\n",
      "Original 6 (limpio, tokenizado): i love programming programmers and programs walking walked walks\n",
      "Sin Stop Words 6: love programming programmers programs walking walked walks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que stopwords esté cargado\n",
    "try:\n",
    "    stop_words_list = stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Descargando 'stopwords' de NLTK...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text) # Tokenizar primero\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_list]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "print(\"3.7 Eliminación de Palabras Vacías\")\n",
    "stop_words_example = \"This is a really great time for the field of AI. It is advancing exponentially.\"\n",
    "print(f\"Original con Stop Words: {stop_words_example}\")\n",
    "print(f\"Sin Stop Words: {remove_stop_words(stop_words_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    # Para aplicar stop words, usualmente se hace después de quitar puntuación y pasar a minúsculas.\n",
    "    temp_text = lowercase_text(text)\n",
    "    temp_text = remove_punctuation(temp_text) # Quitar puntuación antes de tokenizar para stop words\n",
    "    processed_text = remove_stop_words(temp_text)\n",
    "\n",
    "    # Reconstruir un \"original\" comparable para la impresión\n",
    "    original_for_print = lowercase_text(text)\n",
    "    original_for_print = remove_punctuation(original_for_print)\n",
    "    original_for_print_tokenized = \" \".join(word_tokenize(original_for_print))\n",
    "\n",
    "\n",
    "    if original_for_print_tokenized != processed_text:\n",
    "        print(f\"Original {i+1} (limpio, tokenizado): {original_for_print_tokenized}\")\n",
    "        print(f\"Sin Stop Words {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ec8ee",
   "metadata": {},
   "source": [
    "### **3.8 Manejo de Emojis (Handling Emojis)**\n",
    "Los emojis aportan contenido expresivo pero pueden ser un desafío para los modelos.\n",
    "Las opciones incluyen eliminarlos o reemplazarlos por su significado textual. Aquí los eliminaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6028168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 Manejo de Emojis (Eliminación)\n",
      "Original con Emojis: You are very funny 😂😃👍\n",
      "Sin Emojis: You are very funny \n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D 😂\n",
      "Sin Emojis 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    # Patrón de regex amplio para emojis comunes\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print(\"3.8 Manejo de Emojis (Eliminación)\")\n",
    "emoji_example = \"You are very funny 😂😃👍\"\n",
    "print(f\"Original con Emojis: {emoji_example}\")\n",
    "print(f\"Sin Emojis: {remove_emojis(emoji_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_emojis(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Sin Emojis {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbff9c",
   "metadata": {},
   "source": [
    "### **3.9 Tokenización (Tokenization)**\n",
    "Divide el texto en unidades más pequeñas, como palabras (word tokenization) o frases (sentence tokenization).\n",
    "Prepara los datos para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5a67e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 Tokenización\n",
      "Texto Original: Hello world. I am going to Mumbai. This is great!\n",
      "Tokens de Palabras: ['Hello', 'world', '.', 'I', 'am', 'going', 'to', 'Mumbai', '.', 'This', 'is', 'great', '!']\n",
      "Tokens de Frases: ['Hello world.', 'I am going to Mumbai.', 'This is great!']\n",
      "\n",
      "Texto Original para tokenizar: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Tokens de Palabras: ['Hello', 'WORLD', '!', 'This', 'is', 'an', '<', 'B', '>', 'example', '<', '/B', '>', 'text', 'for', 'NLP', 'preprocessing', '.']\n",
      "Tokens de Frases: ['Hello WORLD!', 'This is an <B>example</B> text for NLP preprocessing.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK requiere 'punkt' para tokenizar. Asegurémonos de que está disponible.\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    print(\"Descargando 'punkt' de NLTK...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"3.9 Tokenización\")\n",
    "tokenization_example = \"Hello world. I am going to Mumbai. This is great!\"\n",
    "print(f\"Texto Original: {tokenization_example}\")\n",
    "\n",
    "# Tokenización de palabras\n",
    "word_tokens = word_tokenize(tokenization_example)\n",
    "print(f\"Tokens de Palabras: {word_tokens}\")\n",
    "\n",
    "# Tokenización de frases\n",
    "sentence_tokens = sent_tokenize(tokenization_example)\n",
    "print(f\"Tokens de Frases: {sentence_tokens}\\n\")\n",
    "\n",
    "# Aplicar a una muestra\n",
    "sample_for_tokenization = sample_texts[0] # \"Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\"\n",
    "print(f\"Texto Original para tokenizar: {sample_for_tokenization}\")\n",
    "print(f\"Tokens de Palabras: {word_tokenize(sample_for_tokenization)}\")\n",
    "print(f\"Tokens de Frases: {sent_tokenize(sample_for_tokenization)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d7e9a",
   "metadata": {},
   "source": [
    "### **3.10 Derivación (Stemming)**\n",
    "Reduce las palabras a su forma raíz o \"stem\".\n",
    "Por ejemplo, \"running\", \"runs\", \"ran\" podrían reducirse a \"run\".\n",
    "El \"stem\" resultante no siempre es una palabra real del idioma. NLTK ofrece varios stemmers, como PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393363c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10 Derivación (Stemming)\n",
      "Original para Stemming: walk walks walking walked runner running easily\n",
      "Stemmed: walk walk walk walk runner run easili\n",
      "\n",
      "Original para Stemming (limpio): i love programming programmers and programs walking walked walks\n",
      "Stemmed: i love program programm and program walk walk walk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "print(\"3.10 Derivación (Stemming)\")\n",
    "stemming_example = \"walk walks walking walked runner running easily\"\n",
    "print(f\"Original para Stemming: {stemming_example}\")\n",
    "print(f\"Stemmed: {stem_words_text(stemming_example)}\\n\")\n",
    "\n",
    "stemming_example_2 = sample_texts[5] # \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "# Para un mejor stemming, aplicar después de minúsculas y quitar puntuación\n",
    "temp_text = lowercase_text(stemming_example_2)\n",
    "temp_text = remove_punctuation(temp_text)\n",
    "processed_text = stem_words_text(temp_text)\n",
    "\n",
    "print(f\"Original para Stemming (limpio): {temp_text}\")\n",
    "print(f\"Stemmed: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b893f5",
   "metadata": {},
   "source": [
    "### **3.11 Lematización (Lemmatization)**\n",
    "Similar al stemming, pero reduce las palabras a su forma base significativa (lemma), que es una palabra real del diccionario.\n",
    "Suele ser más lento que el stemming pero puede ser más preciso.\n",
    "WordNetLemmatizer de NLTK es comúnmente usado. Puede tomar un argumento `pos` (Part Of Speech) para mejorar la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d450575",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11 Lematización (Lemmatization)\n",
      "Original para Lematización: He was running and eating at the same time. He has a bad habit of swimming after meals.\n",
      "Lematizado (con pos='v'): he be run and eat at the same time he have a bad habit of swim after meals\n",
      "Lematizado (con pos='n' por defecto): he wa running and eating at the same time he ha a bad habit of swimming after meal\n",
      "\n",
      "Original para Lematización (limpio): i love programming programmers and programs walking walked walks\n",
      "Lematizado (pos='v'): i love program programmers and program walk walk walk\n",
      "Lematizado (pos='n'): i love programming programmer and program walking walked walk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLTK requiere 'wordnet' y 'omw-1.4' para lematizar.\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"test\")\n",
    "except LookupError:\n",
    "    print(\"Descargando 'wordnet' y 'omw-1.4' de NLTK...\")\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet, a veces necesario\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words_text(text, pos_tag='v'): \n",
    "    words = word_tokenize(text)\n",
    "    # Para una lematización más precisa, se necesitaría un etiquetado POS adecuado para cada palabra.\n",
    "    # Aquí usamos un 'pos_tag' general o ninguno (que por defecto es 'n' - noun).\n",
    "    if pos_tag:\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, pos=pos_tag) for word in words]\n",
    "    else:\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "print(\"3.11 Lematización (Lemmatization)\")\n",
    "lemmatization_example = \"He was running and eating at the same time. He has a bad habit of swimming after meals.\"\n",
    "print(f\"Original para Lematización: {lemmatization_example}\")\n",
    "# Limpiar puntuación y pasar a minúsculas para mejor resultado\n",
    "temp_text_lemma = lowercase_text(lemmatization_example)\n",
    "temp_text_lemma = remove_punctuation(temp_text_lemma)\n",
    "\n",
    "print(f\"Lematizado (con pos='v'): {lemmatize_words_text(temp_text_lemma, pos_tag='v')}\")\n",
    "print(f\"Lematizado (con pos='n' por defecto): {lemmatize_words_text(temp_text_lemma, pos_tag=None)}\\n\") # Noun es el default\n",
    "\n",
    "# Otro ejemplo\n",
    "lemmatization_example_2 = sample_texts[5] # \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "temp_text_2 = lowercase_text(lemmatization_example_2)\n",
    "temp_text_2 = remove_punctuation(temp_text_2)\n",
    "processed_text_v = lemmatize_words_text(temp_text_2, pos_tag='v') # Tratar como verbos\n",
    "processed_text_n = lemmatize_words_text(temp_text_2, pos_tag='n') # Tratar como sustantivos (o default)\n",
    "\n",
    "print(f\"Original para Lematización (limpio): {temp_text_2}\")\n",
    "print(f\"Lematizado (pos='v'): {processed_text_v}\")\n",
    "print(f\"Lematizado (pos='n'): {processed_text_n}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ed604",
   "metadata": {},
   "source": [
    "## **4. Conclusiones**\n",
    "\n",
    "Este script ha demostrado las técnicas fundamentales de preprocesamiento de texto en PNL. Cada paso juega un rol en refinar el texto crudo, preparándolo para modelos de aprendizaje automático. La elección y el orden de estas técnicas deben adaptarse a los requisitos específicos de cada proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c726ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del script de demostración de preprocesamiento de texto.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fin del script de demostración de preprocesamiento de texto.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "MADSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
