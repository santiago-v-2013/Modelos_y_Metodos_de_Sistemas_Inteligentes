{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6011fe0b",
   "metadata": {},
   "source": [
    "# **Demostraci贸n de T茅cnicas de Preprocesamiento de Texto en PNL**\n",
    "\n",
    "## **Objetivo del Script:**\n",
    "Este script muestra diversas t茅cnicas comunes de preprocesamiento de texto utilizadas en el Procesamiento del Lenguaje Natural (PNL). Cada t茅cnica se ilustra con texto de ejemplo, aplicando la transformaci贸n y mostrando los resultados. Este script fue adaptado de  \"Complete Guide to Text Preprocessing in NLP\" (https://medium.com/@devangchavan0204/complete-guide-to-text-preprocessing-in-nlp-b4092c104d3e).\n",
    "\n",
    "## **Estructura del Script:**\n",
    "1.  Carga de las librer铆as necesarias y descarga de recursos NLTK (si es necesario).\n",
    "2.  Definici贸n de texto de ejemplo.\n",
    "3.  Aplicaci贸n y visualizaci贸n de t茅cnicas de preprocesamiento:\n",
    "    * Min煤sculas (Lowercasing)\n",
    "    * Eliminar Etiquetas HTML (Remove HTML Tags)\n",
    "    * Eliminar URLs (Remove URLs)\n",
    "    * Eliminar Puntuaci贸n (Remove Punctuations)\n",
    "    * Tratamiento de Palabras de Chat (Chat Word Treatment)\n",
    "    * Correcci贸n Ortogr谩fica (Spelling Correction)\n",
    "    * Eliminaci贸n de Palabras Vac铆as (Removing Stop Words)\n",
    "    * Manejo de Emojis (Handling Emojis)\n",
    "    * Tokenizaci贸n (Tokenization)\n",
    "    * Derivaci贸n (Stemming)\n",
    "    * Lematizaci贸n (Lemmatization)\n",
    "\n",
    "## **Nota sobre NLTK:**\n",
    "Algunas funciones de NLTK requieren la descarga de recursos espec铆ficos (corpus, modelos). Si es la primera vez que usas NLTK para estas tareas, es posible que necesites ejecutar comandos de descarga. Se incluye una celda para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23ee3b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/santiago/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/santiago/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librer铆as cargadas.\n",
      "Recursos NLTK necesarios parecen estar disponibles.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# 1. Carga de Librer铆as y Descarga de Recursos NLTK\n",
    "# ------------------------------------------------------------------------------\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd # Opcional, para mostrar c贸mo se aplicar铆a a un DataFrame\n",
    "\n",
    "# Descomenta y ejecuta las siguientes l铆neas si no tienes los recursos de NLTK\n",
    "# Necesitar谩s hacerlo una vez por instalaci贸n de Python/entorno.\n",
    "nltk.download('punkt') # Para tokenizaci贸n\n",
    "nltk.download('stopwords') # Para palabras vac铆as\n",
    "nltk.download('wordnet') # Para lematizaci贸n\n",
    "nltk.download('omw-1.4') # Necesario para wordnet en algunos casos\n",
    "nltk.download('punkt_tab') # Para tokenizaci贸n de tablas\n",
    "\n",
    "print(\"Librer铆as cargadas.\")\n",
    "try:\n",
    "    # Intenta cargar recursos para verificar si est谩n disponibles\n",
    "    stopwords.words('english')\n",
    "    word_tokenize(\"test\")\n",
    "    PorterStemmer().stem(\"testing\")\n",
    "    WordNetLemmatizer().lemmatize(\"testing\")\n",
    "    print(\"Recursos NLTK necesarios parecen estar disponibles.\")\n",
    "except LookupError as e:\n",
    "    print(f\"Error de NLTK: {e}. Por favor, descomenta y ejecuta las l铆neas nltk.download(...) en el script.\")\n",
    "except Exception as e:\n",
    "    print(f\"Otro error relacionado con NLTK: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c74abd",
   "metadata": {},
   "source": [
    "## **2. Texto de Ejemplo**\n",
    "\n",
    "Definiremos algunas cadenas de texto para aplicar las t茅cnicas de preprocesamiento.\n",
    "Estas muestras incluyen caracter铆sticas que cada paso de preprocesamiento abordar谩."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8f9a98",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto de ejemplo original:\n",
      "1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "6: I love programming, programmers, and programs. Walking, walked, walks.\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\n",
    "    \"Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\",\n",
    "    \"Check out my new blog post at https://example.com/my-post or www.another-example.com.\",\n",
    "    \"OMG! This is GR8! BTW, what's up? CUL8R.\",\n",
    "    \"Thiss is a sentance with some missspellings and errrors. :D \",\n",
    "    \"The quick brown fox jumps over the lazy dog. He was running and eating.\",\n",
    "    \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "]\n",
    "\n",
    "# Tambi茅n podemos crear un DataFrame de Pandas para simular la aplicaci贸n a un conjunto de datos,\n",
    "# como se menciona en el art铆culo de referencia con el dataset IMDB.\n",
    "df_sample = pd.DataFrame(sample_texts, columns=['text'])\n",
    "\n",
    "print(\"Texto de ejemplo original:\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    print(f\"{i+1}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f88749b",
   "metadata": {},
   "source": [
    "## **3. T茅cnicas de Preprocesamiento de Texto**\n",
    "\n",
    "A continuaci贸n, aplicaremos cada t茅cnica de preprocesamiento.\n",
    "No siempre es necesario aplicar todos los pasos; la elecci贸n depende de los requisitos del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c38e8",
   "metadata": {},
   "source": [
    "### **3.1 Min煤sculas (Lowercasing)**\n",
    "Convierte todo el texto a min煤sculas para asegurar uniformidad.\n",
    "Por ejemplo, \"Word\" y \"word\" se tratan como la misma entidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cf32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1 Min煤sculas\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: hello world! this is an <b>example</b> text for nlp preprocessing.\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: omg! this is gr8! btw, what's up? cul8r.\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "Procesado 4: thiss is a sentance with some missspellings and errrors. :d \n",
      "\n",
      "Original 5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "Procesado 5: the quick brown fox jumps over the lazy dog. he was running and eating.\n",
      "\n",
      "Original 6: I love programming, programmers, and programs. Walking, walked, walks.\n",
      "Procesado 6: i love programming, programmers, and programs. walking, walked, walks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower() # String = representacion del texto en el lenguaje de programaci贸n\n",
    "\n",
    "print(\"3.1 Min煤sculas\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = lowercase_text(text)\n",
    "    if text != processed_text: # Mostrar solo si hay cambios\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")\n",
    "\n",
    "# Aplicado al DataFrame de ejemplo:\n",
    "# df_sample['text_lowercased'] = df_sample['text'].apply(lowercase_text)\n",
    "# print(df_sample[['text', 'text_lowercased']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e65f66",
   "metadata": {},
   "source": [
    "### **3.2 Eliminar Etiquetas HTML (Remove HTML Tags)**\n",
    "Las etiquetas HTML, a menudo presentes en datos extra铆dos de la web (scraped data), son irrelevantes para los modelos de ML.\n",
    "Eliminarlas asegura que el modelo se enfoque en el contenido textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b01cd569",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 Eliminar Etiquetas HTML\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: Hello WORLD! This is an example text for NLP preprocessing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>') # Expresi贸n regular para encontrar etiquetas HTML\n",
    "    return pattern.sub(r'', text) # Reemplaza las etiquetas con una cadena vac铆a\n",
    "\n",
    "print(\"3.2 Eliminar Etiquetas HTML\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_html_tags(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08df9",
   "metadata": {},
   "source": [
    "### **3.3 Eliminar URLs (Remove URLs)**\n",
    "Las URLs pueden no aportar informaci贸n valiosa y pueden confundir al modelo.\n",
    "Eliminarlas simplifica los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba33627",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 Eliminar URLs\n",
      "Original URL: For notebook click https://www.kaggle.com/campusx/notebook8223fclabb or visit www.mysite.com\n",
      "Procesado URL: For notebook click [URL] or visit [URL]\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: Check out my new blog post at [URL] or [URL]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    # Patr贸n mejorado para cubrir http, https y www\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'[URL]', text)\n",
    "\n",
    "print(\"3.3 Eliminar URLs\")\n",
    "text_with_url = 'For notebook click https://www.kaggle.com/campusx/notebook8223fclabb or visit www.mysite.com' \n",
    "print(f\"Original URL: {text_with_url}\")\n",
    "print(f\"Procesado URL: {remove_urls(text_with_url)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_urls(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa2f2b",
   "metadata": {},
   "source": [
    "### **3.4 Eliminar Puntuaci贸n (Remove Punctuations)**\n",
    "Elimina caracteres como comas y puntos, enfoc谩ndose en las palabras.\n",
    "Puede ser beneficioso para tareas como el an谩lisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4 Eliminar Puntuaci贸n\n",
      "Original 1: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Procesado 1: Hello WORLD This is an BexampleB text for NLP preprocessing\n",
      "\n",
      "Original 2: Check out my new blog post at https://example.com/my-post or www.another-example.com.\n",
      "Procesado 2: Check out my new blog post at httpsexamplecommypost or wwwanotherexamplecom\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: OMG This is GR8 BTW whats up CUL8R\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "Procesado 4: Thiss is a sentance with some missspellings and errrors D \n",
      "\n",
      "Original 5: The quick brown fox jumps over the lazy dog. He was running and eating.\n",
      "Procesado 5: The quick brown fox jumps over the lazy dog He was running and eating\n",
      "\n",
      "Original 6: I love programming, programmers, and programs. Walking, walked, walks.\n",
      "Procesado 6: I love programming programmers and programs Walking walked walks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    # string.punctuation contiene !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ (Bag of word, tf-idf, word2vec)\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "print(\"3.4 Eliminar Puntuaci贸n\")\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_punctuation(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e830630f",
   "metadata": {},
   "source": [
    "### **3.5 Tratamiento de Palabras de Chat (Chat Word Treatment)**\n",
    "Convierte palabras de chat o jerga (ej. \"FYI\", \"LOL\") a sus formas completas para mejorar la comprensi贸n del modelo.\n",
    "Se utiliza un diccionario para las conversiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8d0e37b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 Tratamiento de Palabras de Chat\n",
      "Original Slang: FYI U R GR8! CUL8R.\n",
      "Procesado Slang: FOR YOUR INFORMATION YOU ARE GREAT! SEE YOU LATER\n",
      "\n",
      "Original 3: OMG! This is GR8! BTW, what's up? CUL8R.\n",
      "Procesado 3: OH MY GOD This is GREAT! BY THE WAY what's up? SEE YOU LATER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diccionario de ejemplo\n",
    "chat_word_dict = {\n",
    "    'AFAIK': 'As Far As I Know', 'ASAP': 'As Soon As Possible',\n",
    "    'BTW': 'By The Way', 'B4': 'Before', 'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FYI': 'For Your Information',\n",
    "    'GG': 'Good Game', 'GR8': 'Great!', 'IMHO': 'In My Honest Opinion',\n",
    "    'LOL': 'Laughing Out Loud', 'OMG': 'Oh My God', 'TTYL': 'Talk To You Later',\n",
    "    'U': 'You', 'R': 'Are'\n",
    "}\n",
    "\n",
    "def chat_conversion(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Considerar la puntuaci贸n pegada a la palabra\n",
    "        word_upper = word.upper()\n",
    "        # Eliminar puntuaci贸n com煤n al final para buscar en el dict\n",
    "        cleaned_word_upper = re.sub(r'[^\\w\\s]$', '', word_upper)\n",
    "\n",
    "        if cleaned_word_upper in chat_word_dict:\n",
    "            # Conservar la capitalizaci贸n original (aproximado)\n",
    "            replacement = chat_word_dict[cleaned_word_upper]\n",
    "            if word.islower():\n",
    "                new_words.append(replacement.lower())\n",
    "            elif word.isupper():\n",
    "                new_words.append(replacement.upper())\n",
    "            elif word[0].isupper():\n",
    "                new_words.append(replacement.capitalize())\n",
    "            else:\n",
    "                new_words.append(replacement)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "\n",
    "print(\"3.5 Tratamiento de Palabras de Chat\")\n",
    "text_slang_example = \"FYI U R GR8! CUL8R.\"\n",
    "print(f\"Original Slang: {text_slang_example}\")\n",
    "print(f\"Procesado Slang: {chat_conversion(text_slang_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = chat_conversion(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Procesado {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba99477",
   "metadata": {},
   "source": [
    "### **3.6 Correcci贸n Ortogr谩fica (Spelling Correction)**\n",
    "Corrige errores tipogr谩ficos e inexactitudes, mejorando la calidad de los datos.\n",
    "Se puede usar la librer铆a `TextBlob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602a0b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6 Correcci贸n Ortogr谩fica\n",
      "Original Incorrecto: Thiss is a sentance with some missspellings and errrors.\n",
      "Corregido (TextBlob): Hiss is a sentence with some missspellings and errors.\n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "Corregido 4: Hiss is a sentence with some missspellings and errors. :D \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def correct_spelling(text):\n",
    "    # TextBlob es 煤til pero puede ser lento en grandes datasets\n",
    "    # y no siempre es perfecto.\n",
    "    text_blob = TextBlob(text)\n",
    "    return str(text_blob.correct())\n",
    "\n",
    "print(\"3.6 Correcci贸n Ortogr谩fica\")\n",
    "incorrect_text_example = 'Thiss is a sentance with some missspellings and errrors.'\n",
    "# Nota: La correcci贸n de TextBlob puede no ser perfecta o puede cambiar palabras correctamente escritas.\n",
    "# Para este ejemplo, mostramos su funcionamiento b谩sico.\n",
    "# En la pr谩ctica, se debe usar con precauci贸n y evaluar su impacto.\n",
    "print(f\"Original Incorrecto: {incorrect_text_example}\")\n",
    "print(f\"Corregido (TextBlob): {correct_spelling(incorrect_text_example)}\\n\")\n",
    "\n",
    "# Aplicar a uno de los textos de muestra que tiene errores\n",
    "for i, text in enumerate(sample_texts):\n",
    "    if \"missspellings\" in text or \"Thiss\" in text: # Aplicar solo a textos con errores conocidos\n",
    "        processed_text = correct_spelling(text)\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Corregido {i+1}: {processed_text}\\n\")\n",
    "        break # Solo un ejemplo para no tardar mucho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8eadd",
   "metadata": {},
   "source": [
    "### **3.7 Eliminaci贸n de Palabras Vac铆as (Removing Stop Words)**\n",
    "Las palabras vac铆as (stop words) son palabras comunes que ayudan a formar oraciones pero aportan poco significado (ej. \"the\", \"is\", \"and\").\n",
    "NLTK proporciona una lista de estas palabras.\n",
    "Este paso no siempre es deseable, por ejemplo, en el etiquetado de Partes de la Oraci贸n (POS tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c9ca79e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 Eliminaci贸n de Palabras Vac铆as\n",
      "Original con Stop Words: This is a really great time for the field of AI. It is advancing exponentially.\n",
      "Sin Stop Words: really great time field AI . advancing exponentially .\n",
      "\n",
      "Original 1 (limpio, tokenizado): hello world this is an bexampleb text for nlp preprocessing\n",
      "Sin Stop Words 1: hello world bexampleb text nlp preprocessing\n",
      "\n",
      "Original 2 (limpio, tokenizado): check out my new blog post at httpsexamplecommypost or wwwanotherexamplecom\n",
      "Sin Stop Words 2: check new blog post httpsexamplecommypost wwwanotherexamplecom\n",
      "\n",
      "Original 3 (limpio, tokenizado): omg this is gr8 btw whats up cul8r\n",
      "Sin Stop Words 3: omg gr8 btw whats cul8r\n",
      "\n",
      "Original 4 (limpio, tokenizado): thiss is a sentance with some missspellings and errrors d \n",
      "Sin Stop Words 4: thiss sentance missspellings errrors \n",
      "\n",
      "Original 5 (limpio, tokenizado): the quick brown fox jumps over the lazy dog he was running and eating\n",
      "Sin Stop Words 5: quick brown fox jumps lazy dog running eating\n",
      "\n",
      "Original 6 (limpio, tokenizado): i love programming programmers and programs walking walked walks\n",
      "Sin Stop Words 6: love programming programmers programs walking walked walks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Asegurarse de que stopwords est茅 cargado\n",
    "try:\n",
    "    stop_words_list = stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Descargando 'stopwords' de NLTK...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stop_words_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text) # Tokenizar primero\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words_list]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "print(\"3.7 Eliminaci贸n de Palabras Vac铆as\")\n",
    "stop_words_example = \"This is a really great time for the field of AI. It is advancing exponentially.\"\n",
    "print(f\"Original con Stop Words: {stop_words_example}\")\n",
    "print(f\"Sin Stop Words: {remove_stop_words(stop_words_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    # Para aplicar stop words, usualmente se hace despu茅s de quitar puntuaci贸n y pasar a min煤sculas.\n",
    "    temp_text = lowercase_text(text)\n",
    "    temp_text = remove_punctuation(temp_text) # Quitar puntuaci贸n antes de tokenizar para stop words\n",
    "    processed_text = remove_stop_words(temp_text)\n",
    "\n",
    "    # Reconstruir un \"original\" comparable para la impresi贸n\n",
    "    original_for_print = lowercase_text(text)\n",
    "    original_for_print = remove_punctuation(original_for_print)\n",
    "    original_for_print_tokenized = \" \".join(word_tokenize(original_for_print))\n",
    "\n",
    "\n",
    "    if original_for_print_tokenized != processed_text:\n",
    "        print(f\"Original {i+1} (limpio, tokenizado): {original_for_print_tokenized}\")\n",
    "        print(f\"Sin Stop Words {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ec8ee",
   "metadata": {},
   "source": [
    "### **3.8 Manejo de Emojis (Handling Emojis)**\n",
    "Los emojis aportan contenido expresivo pero pueden ser un desaf铆o para los modelos.\n",
    "Las opciones incluyen eliminarlos o reemplazarlos por su significado textual. Aqu铆 los eliminaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6028168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8 Manejo de Emojis (Eliminaci贸n)\n",
      "Original con Emojis: You are very funny \n",
      "Sin Emojis: You are very funny \n",
      "\n",
      "Original 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "Sin Emojis 4: Thiss is a sentance with some missspellings and errrors. :D \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    # Patr贸n de regex amplio para emojis comunes\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print(\"3.8 Manejo de Emojis (Eliminaci贸n)\")\n",
    "emoji_example = \"You are very funny \"\n",
    "print(f\"Original con Emojis: {emoji_example}\")\n",
    "print(f\"Sin Emojis: {remove_emojis(emoji_example)}\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    processed_text = remove_emojis(text)\n",
    "    if text != processed_text:\n",
    "        print(f\"Original {i+1}: {text}\")\n",
    "        print(f\"Sin Emojis {i+1}: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbff9c",
   "metadata": {},
   "source": [
    "### **3.9 Tokenizaci贸n (Tokenization)**\n",
    "Divide el texto en unidades m谩s peque帽as, como palabras (word tokenization) o frases (sentence tokenization).\n",
    "Prepara los datos para el an谩lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f5a67e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9 Tokenizaci贸n\n",
      "Texto Original: Hello world. I am going to Mumbai. This is great!\n",
      "Tokens de Palabras: ['Hello', 'world', '.', 'I', 'am', 'going', 'to', 'Mumbai', '.', 'This', 'is', 'great', '!']\n",
      "Tokens de Frases: ['Hello world.', 'I am going to Mumbai.', 'This is great!']\n",
      "\n",
      "Texto Original para tokenizar: Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\n",
      "Tokens de Palabras: ['Hello', 'WORLD', '!', 'This', 'is', 'an', '<', 'B', '>', 'example', '<', '/B', '>', 'text', 'for', 'NLP', 'preprocessing', '.']\n",
      "Tokens de Frases: ['Hello WORLD!', 'This is an <B>example</B> text for NLP preprocessing.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK requiere 'punkt' para tokenizar. Asegur茅monos de que est谩 disponible.\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    print(\"Descargando 'punkt' de NLTK...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"3.9 Tokenizaci贸n\")\n",
    "tokenization_example = \"Hello world. I am going to Mumbai. This is great!\"\n",
    "print(f\"Texto Original: {tokenization_example}\")\n",
    "\n",
    "# Tokenizaci贸n de palabras\n",
    "word_tokens = word_tokenize(tokenization_example)\n",
    "print(f\"Tokens de Palabras: {word_tokens}\")\n",
    "\n",
    "# Tokenizaci贸n de frases\n",
    "sentence_tokens = sent_tokenize(tokenization_example)\n",
    "print(f\"Tokens de Frases: {sentence_tokens}\\n\")\n",
    "\n",
    "# Aplicar a una muestra\n",
    "sample_for_tokenization = sample_texts[0] # \"Hello WORLD! This is an <B>example</B> text for NLP preprocessing.\"\n",
    "print(f\"Texto Original para tokenizar: {sample_for_tokenization}\")\n",
    "print(f\"Tokens de Palabras: {word_tokenize(sample_for_tokenization)}\")\n",
    "print(f\"Tokens de Frases: {sent_tokenize(sample_for_tokenization)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d7e9a",
   "metadata": {},
   "source": [
    "### **3.10 Derivaci贸n (Stemming)**\n",
    "Reduce las palabras a su forma ra铆z o \"stem\".\n",
    "Por ejemplo, \"running\", \"runs\", \"ran\" podr铆an reducirse a \"run\".\n",
    "El \"stem\" resultante no siempre es una palabra real del idioma. NLTK ofrece varios stemmers, como PorterStemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "393363c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10 Derivaci贸n (Stemming)\n",
      "Original para Stemming: walk walks walking walked runner running easily\n",
      "Stemmed: walk walk walk walk runner run easili\n",
      "\n",
      "Original para Stemming (limpio): i love programming programmers and programs walking walked walks\n",
      "Stemmed: i love program programm and program walk walk walk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_words_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "print(\"3.10 Derivaci贸n (Stemming)\")\n",
    "stemming_example = \"walk walks walking walked runner running easily\"\n",
    "print(f\"Original para Stemming: {stemming_example}\")\n",
    "print(f\"Stemmed: {stem_words_text(stemming_example)}\\n\")\n",
    "\n",
    "stemming_example_2 = sample_texts[5] # \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "# Para un mejor stemming, aplicar despu茅s de min煤sculas y quitar puntuaci贸n\n",
    "temp_text = lowercase_text(stemming_example_2)\n",
    "temp_text = remove_punctuation(temp_text)\n",
    "processed_text = stem_words_text(temp_text)\n",
    "\n",
    "print(f\"Original para Stemming (limpio): {temp_text}\")\n",
    "print(f\"Stemmed: {processed_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b893f5",
   "metadata": {},
   "source": [
    "### **3.11 Lematizaci贸n (Lemmatization)**\n",
    "Similar al stemming, pero reduce las palabras a su forma base significativa (lemma), que es una palabra real del diccionario.\n",
    "Suele ser m谩s lento que el stemming pero puede ser m谩s preciso.\n",
    "WordNetLemmatizer de NLTK es com煤nmente usado. Puede tomar un argumento `pos` (Part Of Speech) para mejorar la precisi贸n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d450575",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11 Lematizaci贸n (Lemmatization)\n",
      "Original para Lematizaci贸n: He was running and eating at the same time. He has a bad habit of swimming after meals.\n",
      "Lematizado (con pos='v'): he be run and eat at the same time he have a bad habit of swim after meals\n",
      "Lematizado (con pos='n' por defecto): he wa running and eating at the same time he ha a bad habit of swimming after meal\n",
      "\n",
      "Original para Lematizaci贸n (limpio): i love programming programmers and programs walking walked walks\n",
      "Lematizado (pos='v'): i love program programmers and program walk walk walk\n",
      "Lematizado (pos='n'): i love programming programmer and program walking walked walk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NLTK requiere 'wordnet' y 'omw-1.4' para lematizar.\n",
    "try:\n",
    "    WordNetLemmatizer().lemmatize(\"test\")\n",
    "except LookupError:\n",
    "    print(\"Descargando 'wordnet' y 'omw-1.4' de NLTK...\")\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet, a veces necesario\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words_text(text, pos_tag='v'): \n",
    "    words = word_tokenize(text)\n",
    "    # Para una lematizaci贸n m谩s precisa, se necesitar铆a un etiquetado POS adecuado para cada palabra.\n",
    "    # Aqu铆 usamos un 'pos_tag' general o ninguno (que por defecto es 'n' - noun).\n",
    "    if pos_tag:\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, pos=pos_tag) for word in words]\n",
    "    else:\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "print(\"3.11 Lematizaci贸n (Lemmatization)\")\n",
    "lemmatization_example = \"He was running and eating at the same time. He has a bad habit of swimming after meals.\"\n",
    "print(f\"Original para Lematizaci贸n: {lemmatization_example}\")\n",
    "# Limpiar puntuaci贸n y pasar a min煤sculas para mejor resultado\n",
    "temp_text_lemma = lowercase_text(lemmatization_example)\n",
    "temp_text_lemma = remove_punctuation(temp_text_lemma)\n",
    "\n",
    "print(f\"Lematizado (con pos='v'): {lemmatize_words_text(temp_text_lemma, pos_tag='v')}\")\n",
    "print(f\"Lematizado (con pos='n' por defecto): {lemmatize_words_text(temp_text_lemma, pos_tag=None)}\\n\") # Noun es el default\n",
    "\n",
    "# Otro ejemplo\n",
    "lemmatization_example_2 = sample_texts[5] # \"I love programming, programmers, and programs. Walking, walked, walks.\"\n",
    "temp_text_2 = lowercase_text(lemmatization_example_2)\n",
    "temp_text_2 = remove_punctuation(temp_text_2)\n",
    "processed_text_v = lemmatize_words_text(temp_text_2, pos_tag='v') # Tratar como verbos\n",
    "processed_text_n = lemmatize_words_text(temp_text_2, pos_tag='n') # Tratar como sustantivos (o default)\n",
    "\n",
    "print(f\"Original para Lematizaci贸n (limpio): {temp_text_2}\")\n",
    "print(f\"Lematizado (pos='v'): {processed_text_v}\")\n",
    "print(f\"Lematizado (pos='n'): {processed_text_n}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ed604",
   "metadata": {},
   "source": [
    "## **4. Conclusiones**\n",
    "\n",
    "Este script ha demostrado las t茅cnicas fundamentales de preprocesamiento de texto en PNL. Cada paso juega un rol en refinar el texto crudo, prepar谩ndolo para modelos de aprendizaje autom谩tico. La elecci贸n y el orden de estas t茅cnicas deben adaptarse a los requisitos espec铆ficos de cada proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c726ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin del script de demostraci贸n de preprocesamiento de texto.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fin del script de demostraci贸n de preprocesamiento de texto.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "MADSI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
